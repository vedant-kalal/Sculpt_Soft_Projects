{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e6bccf",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION TECHNIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16025ca4",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation (Explained)\n",
    "\n",
    "**What is it?**  \n",
    "\n",
    "K-Fold Cross Validation is a way to check how well a machine learning model will perform on **unseen data**.  \n",
    "\n",
    "Instead of keeping *one* fixed test set, we split the dataset into **K equal parts (folds)**.  \n",
    "- In each round, **1 fold** is used as the **test set**, and the other **K-1 folds** are used for **training**.  \n",
    "- This repeats **K times**, so every sample gets a chance to be in the test set exactly once.  \n",
    "- Finally, we average the performance across all folds.\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Example: 12 samples with 4-fold CV\n",
    "Imagine our dataset has 12 points (0‚Äì11). If we split into **4 folds**, each fold will have **3 samples**:\n",
    "\n",
    "```\n",
    "Fold 1: Test = [0,1,2]      | Train = [3..11]\n",
    "Fold 2: Test = [3,4,5]      | Train = [0..2, 6..11]\n",
    "Fold 3: Test = [6,7,8]      | Train = [0..5, 9..11]\n",
    "Fold 4: Test = [9,10,11]    | Train = [0..8]\n",
    "```\n",
    "\n",
    "So each sample is tested exactly once.  \n",
    "This avoids the risk of testing on the same data you trained on.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è Why is this useful?\n",
    "- More **reliable estimate** of model accuracy (less dependent on a lucky/unlucky test split).  \n",
    "- Helps spot problems like **overfitting**.  \n",
    "- Gives mean + standard deviation ‚Üí so you know if your model is **stable**.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Steps\n",
    "1. Choose number of folds `K`.  \n",
    "2. Split dataset into K parts.  \n",
    "3. For each fold:  \n",
    "   - Train on K-1 folds.  \n",
    "   - Test on the remaining fold.  \n",
    "4. Collect performance scores.  \n",
    "5. Average them ‚Üí this is your **cross-validation score**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Disadvantages & When Not to Use K-Fold Cross Validation\n",
    "\n",
    "- **Computationally expensive:** Training and evaluating the model K times can be slow, especially for large datasets or complex models.\n",
    "- **Not ideal for time series data:** K-Fold randomly splits data, which can break the temporal order. For time series, use techniques like TimeSeriesSplit or walk-forward validation.\n",
    "- **Imbalanced data:** If classes are not evenly distributed, some folds may have very few samples of a class. Use StratifiedKFold for classification tasks with imbalanced classes.\n",
    "- **Data leakage risk:** If data points are not independent (e.g., multiple rows per user), splitting randomly can leak information between train and test sets. GroupKFold or custom splits are better in such cases.\n",
    "- **Small datasets:** On very small datasets, the variance between folds can be high, making results less reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b3cbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn cross_val_score scores: [1.00,1.00,0.33,0.00]\n",
      "Sklearn Mean accuracy: 0.5833333333333334\n",
      "Sklearn Std deviation: 0.4330127018922193\n"
     ]
    }
   ],
   "source": [
    "# Example: K-Fold Cross Validation in Python (Manual and sklearn)\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "np.set_printoptions(precision=2, suppress=True, floatmode='fixed', linewidth=100)\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=12, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "kf = KFold(n_splits=4, shuffle=False)\n",
    "model = LogisticRegression()\n",
    "scores = cross_val_score(model, X, y, cv=kf)# cd means which cross-validation method to use\n",
    "print(\"Sklearn cross_val_score scores:\", np.array2string(scores, separator=','))\n",
    "print(\"Sklearn Mean accuracy:\", np.mean(scores))\n",
    "print(\"Sklearn Std deviation:\", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c9a06",
   "metadata": {},
   "source": [
    "## 2). Hold-Out Method (Explained)\n",
    "\n",
    "**What is it?**  \n",
    "The Hold-Out method is the simplest way to evaluate a machine learning model. You split your dataset into two (sometimes three) parts:\n",
    "- **Training set:** Used to train the model.\n",
    "- **Test set:** Used to evaluate the model's performance on unseen data.\n",
    "- (Optional) **Validation set:** Used to tune model parameters before the final test.\n",
    "\n",
    "---\n",
    "\n",
    "### \udd0e Example: 12 samples with Hold-Out (8 train, 4 test)\n",
    "Imagine our dataset has 12 points (0‚Äì11). If we use an 8/4 split:\n",
    "\n",
    "```\n",
    "Train = [0,1,2,3,4,5,6,7]   | Test = [8,9,10,11]\n",
    "```\n",
    "\n",
    "Or, visually:\n",
    "\n",
    "```\n",
    "[Train, Train, Train, Train, Train, Train, Train, Train, Test, Test, Test, Test]\n",
    "[   0 ,    1 ,    2 ,    3 ,    4 ,    5 ,    6 ,    7 ,   8 ,   9 ,  10 , 11 ]\n",
    "```\n",
    "\n",
    "So only the last 4 samples are used for testing, the rest for training. (The split can be random or sequential.)\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83düõ†Ô∏è Steps\n",
    "1. Shuffle the dataset (optional, but recommended).\n",
    "2. Split the data into training and test sets (commonly 70/30, 80/20, or 60/20/20 for train/validation/test).\n",
    "3. Train the model on the training set.\n",
    "4. Evaluate the model on the test set.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "- **Simple and fast:** Only one split and one training/testing cycle.\n",
    "- **Good for very large datasets:** When you have lots of data, a single split is often enough.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Disadvantages\n",
    "- **High variance:** Results depend heavily on how the data is split. A \"lucky\" or \"unlucky\" split can give misleading results.\n",
    "- **Not ideal for small datasets:** You might waste valuable data for testing instead of training.\n",
    "- **No estimate of model stability:** You only get one performance score, not an average or standard deviation.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "- The Hold-Out method is quick and easy, but less reliable for small datasets or when you want a robust estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "803e2dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [0 1 2 3 4 5 6 7]\n",
      "Test indices: [ 8  9 10 11]\n",
      "Test set predictions: [1 1 0 1]\n",
      "Test set true labels: [1 0 0 0]\n",
      "Test set accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Example: Hold-Out Method in Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=12, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split into 8 train and 4 test samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=4, random_state=42, shuffle=False)\n",
    "\n",
    "# Show which samples are in train/test\n",
    "print(\"Train indices:\", np.arange(len(X_train)))\n",
    "print(\"Test indices:\", np.arange(len(X_train), len(X)))\n",
    "\n",
    "# Train and evaluate\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Test set predictions:\", y_pred)\n",
    "print(\"Test set true labels:\", y_test)\n",
    "print(\"Test set accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2feff1",
   "metadata": {},
   "source": [
    "## 3). Stratified K-Fold Cross-Validation (Explained)\n",
    "\n",
    "**What is it?**  \n",
    "Stratified K-Fold Cross-Validation is a variation of K-Fold that ensures each fold has (as much as possible) the same proportion of each class label as the full dataset. This is especially important for classification problems with imbalanced classes.\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Visual Example: 12 samples, 2 classes (0 and 1), 4-fold stratified\n",
    "Suppose our dataset labels are:\n",
    "\n",
    "```\n",
    "Labels: [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\n",
    "```\n",
    "\n",
    "- **Normal K-Fold** might split the data like this (class balance not guaranteed):\n",
    "\n",
    "```\n",
    "Fold 1: [0, 0, 0]   (all class 0)\n",
    "Fold 2: [0, 1, 1]   (mixed)\n",
    "Fold 3: [1, 1, 0]   (mixed)\n",
    "Fold 4: [0, 1, 1]   (mixed)\n",
    "```\n",
    "\n",
    "- **Stratified K-Fold** tries to keep the class ratio in each fold similar to the whole dataset:\n",
    "\n",
    "```\n",
    "Fold 1: [0, 1, 0]\n",
    "Fold 2: [0, 1, 1]\n",
    "Fold 3: [0, 1, 1]\n",
    "Fold 4: [0, 1, 0]\n",
    "```\n",
    "\n",
    "So, if the dataset is 50% class 0 and 50% class 1, each fold will be close to 50/50.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Steps\n",
    "1. Choose number of folds `K`.\n",
    "2. Split the dataset so each fold has a similar class distribution as the whole dataset.\n",
    "3. For each fold:\n",
    "   - Train on K-1 folds.\n",
    "   - Test on the remaining fold.\n",
    "4. Collect and average performance scores.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Difference: K-Fold vs. Stratified K-Fold\n",
    "- **K-Fold:** Splits data into K parts randomly, class balance in each fold is not guaranteed.\n",
    "- **Stratified K-Fold:** Splits data so each fold has a similar class distribution as the full dataset (better for imbalanced data).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ When to use Stratified K-Fold?\n",
    "- When you have **classification problems** (especially with imbalanced classes).\n",
    "- When you want more reliable and fair evaluation across all classes.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "- Use Stratified K-Fold for classification tasks to avoid folds with missing or underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4695e180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 0 1 1 1 0 1 1 1 0 0 0]\n",
      "Fold 1 - Test indices: [0 1 2], Test labels: [0 0 1]\n",
      "  Accuracy: 1.00\n",
      "Fold 2 - Test indices: [3 5 9], Test labels: [1 0 0]\n",
      "  Accuracy: 0.67\n",
      "Fold 3 - Test indices: [ 4  6 10], Test labels: [1 1 0]\n",
      "  Accuracy: 0.67\n",
      "Fold 4 - Test indices: [ 7  8 11], Test labels: [1 1 0]\n",
      "  Accuracy: 0.67\n",
      "Stratified K-Fold scores: [1.00,0.67,0.67,0.67]\n",
      "Mean accuracy: 0.7499999999999999\n",
      "Std deviation: 0.14433756729740646\n"
     ]
    }
   ],
   "source": [
    "# Example: Stratified K-Fold Cross-Validation in Python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Create a synthetic imbalanced dataset\n",
    "X, y = make_classification(n_samples=12, n_features=2, n_informative=2, n_redundant=0, n_classes=2, weights=[0.5, 0.5], random_state=42)\n",
    "\n",
    "# Show class distribution\n",
    "print(\"Labels:\", y)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=False)\n",
    "fold = 1\n",
    "scores = []\n",
    "for train_idx, test_idx in skf.split(X, y):\n",
    "    print(f\"Fold {fold} - Test indices: {test_idx}, Test labels: {y[test_idx]}\")\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"  Accuracy: {acc:.2f}\")\n",
    "    scores.append(acc)\n",
    "    fold += 1\n",
    "print(\"Stratified K-Fold scores:\", np.array2string(np.array(scores), separator=','))\n",
    "print(\"Mean accuracy:\", np.mean(scores))\n",
    "print(\"Std deviation:\", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e89c1",
   "metadata": {},
   "source": [
    "## 4). Leave-One-Out Cross-Validation (LOO or LOOCV)\n",
    "\n",
    "**What is it?**  \n",
    "Leave-One-Out Cross-Validation is an extreme case of K-Fold where the number of folds equals the number of samples in the dataset. Each sample is used once as a test set (of size 1), and the rest as the training set.\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Visual Example: 5 samples\n",
    "Suppose our dataset has 5 samples (0‚Äì4):\n",
    "\n",
    "```\n",
    "Iteration 1: Test = [0] | Train = [1,2,3,4]\n",
    "Iteration 2: Test = [1] | Train = [0,2,3,4]\n",
    "Iteration 3: Test = [2] | Train = [0,1,3,4]\n",
    "Iteration 4: Test = [3] | Train = [0,1,2,4]\n",
    "Iteration 5: Test = [4] | Train = [0,1,2,3]\n",
    "```\n",
    "\n",
    "So, each sample is tested exactly once, and the model is trained on all the others each time.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Steps\n",
    "1. For each sample in the dataset:\n",
    "   - Use that sample as the test set.\n",
    "   - Use all other samples as the training set.\n",
    "   - Train and evaluate the model.\n",
    "2. Collect all performance scores and average them.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "- **Maximizes training data:** Each model is trained on as much data as possible (all but one sample).\n",
    "- **No randomness:** Every possible train/test split is used.\n",
    "- **Good for very small datasets:** Makes the most of limited data.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Disadvantages\n",
    "- **Very slow for large datasets:** Number of models trained = number of samples.\n",
    "- **High variance:** Each test set is just one sample, so results can be noisy.\n",
    "- **Not practical for big data.**\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "- Leave-One-Out is best for small datasets where you want to use every sample for both training and testing, but is too slow for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63d99a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Test index: 0, Test label: 0\n",
      "  Prediction: 0, Accuracy: 1.00\n",
      "Iteration 2 - Test index: 1, Test label: 1\n",
      "  Prediction: 1, Accuracy: 1.00\n",
      "Iteration 3 - Test index: 2, Test label: 0\n",
      "  Prediction: 1, Accuracy: 0.00\n",
      "Iteration 4 - Test index: 3, Test label: 0\n",
      "  Prediction: 0, Accuracy: 1.00\n",
      "Iteration 5 - Test index: 4, Test label: 1\n",
      "  Prediction: 0, Accuracy: 0.00\n",
      "Iteration 6 - Test index: 5, Test label: 1\n",
      "  Prediction: 0, Accuracy: 0.00\n",
      "Iteration 7 - Test index: 6, Test label: 0\n",
      "  Prediction: 0, Accuracy: 1.00\n",
      "Iteration 8 - Test index: 7, Test label: 1\n",
      "  Prediction: 1, Accuracy: 1.00\n",
      "LOO scores: [1.00,1.00,0.00,1.00,0.00,0.00,1.00,1.00]\n",
      "Mean accuracy: 0.625\n",
      "Std deviation: 0.4841229182759271\n"
     ]
    }
   ],
   "source": [
    "# Example: Leave-One-Out Cross-Validation in Python\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Create a small synthetic dataset\n",
    "X, y = make_classification(n_samples=8, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "scores = []\n",
    "iteration = 1\n",
    "for train_idx, test_idx in loo.split(X):\n",
    "    print(f\"Iteration {iteration} - Test index: {test_idx[0]}, Test label: {y[test_idx][0]}\")\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"  Prediction: {y_pred[0]}, Accuracy: {acc:.2f}\")\n",
    "    scores.append(acc)\n",
    "    iteration += 1\n",
    "print(\"LOO scores:\", np.array2string(np.array(scores), separator=','))\n",
    "print(\"Mean accuracy:\", np.mean(scores))\n",
    "print(\"Std deviation:\", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf56d1",
   "metadata": {},
   "source": [
    "## 5). Leave-P-Out Cross-Validation (LPOCV)\n",
    "\n",
    "**What is it?**  \n",
    "Leave-P-Out Cross-Validation is a generalization of Leave-One-Out. Instead of leaving out one sample at a time, you leave out every possible combination of `P` samples as the test set, and use the remaining samples for training. This is repeated for all possible combinations.\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Visual Example: 4 samples, P=2\n",
    "Suppose our dataset has 4 samples (0‚Äì3) and we set P=2:\n",
    "\n",
    "All possible ways to leave out 2 samples:\n",
    "```\n",
    "Iteration 1: Test = [0,1] | Train = [2,3]\n",
    "Iteration 2: Test = [0,2] | Train = [1,3]\n",
    "Iteration 3: Test = [0,3] | Train = [1,2]\n",
    "Iteration 4: Test = [1,2] | Train = [0,3]\n",
    "Iteration 5: Test = [1,3] | Train = [0,2]\n",
    "Iteration 6: Test = [2,3] | Train = [0,1]\n",
    "```\n",
    "\n",
    "So, for each possible pair of samples, those are used as the test set, and the rest for training.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Steps\n",
    "1. For every possible combination of `P` samples:\n",
    "   - Use those `P` samples as the test set.\n",
    "   - Use the remaining samples as the training set.\n",
    "   - Train and evaluate the model.\n",
    "2. Collect all performance scores and average them.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "- **Very thorough:** Every possible way to split out `P` samples is tested.\n",
    "- **Good for very small datasets:** Makes the most of limited data.\n",
    "- **No randomness:** All possible splits are used.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Disadvantages\n",
    "- **Extremely slow for large datasets or large P:** The number of combinations grows rapidly (combinatorial explosion).\n",
    "- **Not practical for big data.**\n",
    "- **High computational cost:** Not used in practice for anything but very small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "- Leave-P-Out is a very exhaustive method, best for small datasets and small values of P. For larger datasets, it quickly becomes impractical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac7a2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Test indices: [0 1], Test labels: [0 0]\n",
      "  Predictions: [1 1], Accuracy: 0.00\n",
      "Iteration 2 - Test indices: [0 2], Test labels: [0 0]\n",
      "  Predictions: [1 1], Accuracy: 0.00\n",
      "Iteration 3 - Test indices: [0 3], Test labels: [0 1]\n",
      "  Predictions: [0 1], Accuracy: 1.00\n",
      "Iteration 4 - Test indices: [0 4], Test labels: [0 1]\n",
      "  Predictions: [0 1], Accuracy: 1.00\n",
      "Iteration 5 - Test indices: [0 5], Test labels: [0 1]\n",
      "  Predictions: [0 0], Accuracy: 0.50\n",
      "Iteration 6 - Test indices: [1 2], Test labels: [0 0]\n",
      "  Predictions: [0 1], Accuracy: 0.50\n",
      "Iteration 7 - Test indices: [1 3], Test labels: [0 1]\n",
      "  Predictions: [0 1], Accuracy: 1.00\n",
      "Iteration 8 - Test indices: [1 4], Test labels: [0 1]\n",
      "  Predictions: [0 1], Accuracy: 1.00\n",
      "Iteration 9 - Test indices: [1 5], Test labels: [0 1]\n",
      "  Predictions: [0 0], Accuracy: 0.50\n",
      "Iteration 10 - Test indices: [2 3], Test labels: [0 1]\n",
      "  Predictions: [0 1], Accuracy: 1.00\n",
      "Iteration 11 - Test indices: [2 4], Test labels: [0 1]\n",
      "  Predictions: [0 1], Accuracy: 1.00\n",
      "Iteration 12 - Test indices: [2 5], Test labels: [0 1]\n",
      "  Predictions: [0 0], Accuracy: 0.50\n",
      "Iteration 13 - Test indices: [3 4], Test labels: [1 1]\n",
      "  Predictions: [1 1], Accuracy: 1.00\n",
      "Iteration 14 - Test indices: [3 5], Test labels: [1 1]\n",
      "  Predictions: [0 0], Accuracy: 0.00\n",
      "Iteration 15 - Test indices: [4 5], Test labels: [1 1]\n",
      "  Predictions: [1 0], Accuracy: 0.50\n",
      "LPOCV scores: [0.00,0.00,1.00,1.00,0.50,0.50,1.00,1.00,0.50,1.00,1.00,0.50,1.00,0.00,0.50]\n",
      "Mean accuracy: 0.6333333333333333\n",
      "Std deviation: 0.38586123009300755\n"
     ]
    }
   ],
   "source": [
    "# Example: Leave-P-Out Cross-Validation in Python (P=2)\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Create a very small synthetic dataset\n",
    "X, y = make_classification(n_samples=6, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "lpo = LeavePOut(p=2)\n",
    "scores = []\n",
    "iteration = 1\n",
    "for train_idx, test_idx in lpo.split(X):\n",
    "    print(f\"Iteration {iteration} - Test indices: {test_idx}, Test labels: {y[test_idx]}\")\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"  Predictions: {y_pred}, Accuracy: {acc:.2f}\")\n",
    "    scores.append(acc)\n",
    "    iteration += 1\n",
    "print(\"LPOCV scores:\", np.array2string(np.array(scores), separator=','))\n",
    "print(\"Mean accuracy:\", np.mean(scores))\n",
    "print(\"Std deviation:\", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b87308",
   "metadata": {},
   "source": [
    "## 6). Monte Carlo Cross-Validation (Shuffle-Split)\n",
    "\n",
    "**What is it?**  \n",
    "Monte Carlo Cross-Validation (also called Shuffle-Split) is a resampling method where the dataset is randomly split into training and test sets multiple times. Each split is independent, and the process is repeated for a specified number of iterations.\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Visual Example: 8 samples, 3 iterations, 75% train / 25% test\n",
    "Suppose our dataset has 8 samples (0‚Äì7). For each iteration, we randomly split the data:\n",
    "\n",
    "```\n",
    "Iteration 1: Train = [0,2,3,4,5,7] | Test = [1,6]\n",
    "Iteration 2: Train = [1,2,4,5,6,7] | Test = [0,3]\n",
    "Iteration 3: Train = [0,1,2,3,5,6] | Test = [4,7]\n",
    "```\n",
    "\n",
    "- The splits are random and can overlap.\n",
    "- Each sample may appear in the test set multiple times, or not at all.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Steps\n",
    "1. Choose the number of iterations and the train/test split ratio.\n",
    "2. For each iteration:\n",
    "   - Randomly split the data into training and test sets.\n",
    "   - Train and evaluate the model.\n",
    "3. Collect all performance scores and average them.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "- **Flexible:** You can control the number of splits and the size of train/test sets.\n",
    "- **Good for small to medium datasets:** More robust than a single hold-out split.\n",
    "- **Less computationally expensive than exhaustive methods (like LPOCV).**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Disadvantages\n",
    "- **Randomness:** Some samples may never be in the test set, others may be in the test set multiple times.\n",
    "- **Not as systematic as K-Fold:** Not all samples are guaranteed to be tested exactly once.\n",
    "- **Results can vary between runs unless you set a random seed.**\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "- Monte Carlo Cross-Validation is a flexible, randomized method for estimating model performance, especially useful when you want to repeat random splits many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f9b885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Train indices: [0 7 2 4 3 6], Test indices: [1 5]\n",
      "  Predictions: [1 0], True: [1 1], Accuracy: 0.50\n",
      "Iteration 2 - Train indices: [0 4 5 2 1 6], Test indices: [3 7]\n",
      "  Predictions: [1 1], True: [0 1], Accuracy: 0.50\n",
      "Iteration 3 - Train indices: [3 1 4 5 2 7], Test indices: [0 6]\n",
      "  Predictions: [1 1], True: [0 0], Accuracy: 0.00\n",
      "ShuffleSplit scores: [0.50,0.50,0.00]\n",
      "Mean accuracy: 0.3333333333333333\n",
      "Std deviation: 0.23570226039551584\n"
     ]
    }
   ],
   "source": [
    "# Example: Monte Carlo Cross-Validation (Shuffle-Split) in Python\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Create a small synthetic dataset\n",
    "X, y = make_classification(n_samples=8, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "ss = ShuffleSplit(n_splits=3, test_size=2, random_state=42)\n",
    "scores = []\n",
    "iteration = 1\n",
    "for train_idx, test_idx in ss.split(X):\n",
    "    print(f\"Iteration {iteration} - Train indices: {train_idx}, Test indices: {test_idx}\")\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"  Predictions: {y_pred}, True: {y_test}, Accuracy: {acc:.2f}\")\n",
    "    scores.append(acc)\n",
    "    iteration += 1\n",
    "print(\"ShuffleSplit scores:\", np.array2string(np.array(scores), separator=','))\n",
    "print(\"Mean accuracy:\", np.mean(scores))\n",
    "print(\"Std deviation:\", np.std(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
