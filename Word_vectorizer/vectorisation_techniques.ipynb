{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae673c5",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "**Definition:**\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document (term frequency), but is offset by how common the word is across all documents (inverse document frequency). TF-IDF is widely used in information retrieval, text mining, and natural language processing to transform text into meaningful numerical features for machine learning algorithms.\n",
    "\n",
    "- **TF (Term Frequency):** Measures how frequently a term appears in a document.\n",
    "- **IDF (Inverse Document Frequency):** Measures how unique or rare a term is across all documents in the corpus.\n",
    "- **TF-IDF:** The product of TF and IDF, giving higher scores to terms that are frequent in a document but rare in the corpus.\n",
    "\n",
    "**Key idea:**\n",
    "- Common words (like \"the\", \"is\") get low scores.\n",
    "- Words that are frequent in a document but rare in the corpus get high scores.\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Formulas\n",
    "- **Term Frequency (TF):**\n",
    "  - Measures how frequently a term appears in a document.\n",
    "  - $TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total terms in document } d}$\n",
    "\n",
    "- **Inverse Document Frequency (IDF):**\n",
    "  - Measures how important a term is in the whole corpus.\n",
    "  - $IDF(t) = \\log_{10}\\left(\\frac{N}{DF(t)}\\right)$ (manual calculation, log base 10, no offset)\n",
    "    - $N$ = total number of documents\n",
    "    - $DF(t)$ = number of documents containing term $t$\n",
    "\n",
    "- **TF-IDF:**\n",
    "  - $TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)$\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Note on scikit-learn's TfidfVectorizer\n",
    "- By default, scikit-learn uses the natural logarithm (log base e) and adds 1 to the IDF:\n",
    "  - $IDF_{sklearn}(t) = \\log_e\\left(\\frac{N}{DF(t)}\\right) + 1$\n",
    "- This means even very common words (like \"the\") get a nonzero value in the code output.\n",
    "- The manual example above uses log base 10 and no offset, so common words get 0.\n",
    "- This is why the TF-IDF values in the code and the manual table may differ.\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Example Documents\n",
    "Suppose we have 3 documents:\n",
    "- d1: \"the cat sat\"\n",
    "- d2: \"the dog sat\"\n",
    "- d3: \"the cat ate\"\n",
    "\n",
    "### Step 1: Build the Vocabulary\n",
    "All unique terms: **the, cat, sat, dog, ate**\n",
    "\n",
    "### Step 2: Term Frequency Table\n",
    "| Term | d1 | d2 | d3 |\n",
    "|------|----|----|----|\n",
    "| the  |  1 |  1 |  1 |\n",
    "| cat  |  1 |  0 |  1 |\n",
    "| sat  |  1 |  1 |  0 |\n",
    "| dog  |  0 |  1 |  0 |\n",
    "| ate  |  0 |  0 |  1 |\n",
    "\n",
    "Each document has 3 words, so $TF(t, d) = \\frac{\\text{count}}{3}$\n",
    "\n",
    "| Term | d1 (TF) | d2 (TF) | d3 (TF) |\n",
    "|------|---------|---------|---------|\n",
    "| the  | 1/3     | 1/3     | 1/3     |\n",
    "| cat  | 1/3     | 0       | 1/3     |\n",
    "| sat  | 1/3     | 1/3     | 0       |\n",
    "| dog  | 0       | 1/3     | 0       |\n",
    "| ate  | 0       | 0       | 1/3     |\n",
    "\n",
    "### Step 3: Document Frequency (DF) and IDF\n",
    "- the: appears in d1, d2, d3 → DF = 3\n",
    "- cat: d1, d3 → DF = 2\n",
    "- sat: d1, d2 → DF = 2\n",
    "- dog: d2 → DF = 1\n",
    "- ate: d3 → DF = 1\n",
    "\n",
    "$N = 3$ (number of documents)\n",
    "\n",
    "| Term | DF | IDF = $\\log_{10}(N/DF)$ |\n",
    "|------|----|------------------------|\n",
    "| the  | 3  | $\\log_{10}(3/3)=0$     |\n",
    "| cat  | 2  | $\\log_{10}(3/2)=0.176$ |\n",
    "| sat  | 2  | $\\log_{10}(3/2)=0.176$ |\n",
    "| dog  | 1  | $\\log_{10}(3/1)=0.477$ |\n",
    "| ate  | 1  | $\\log_{10}(3/1)=0.477$ |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: TF-IDF Calculation\n",
    "$TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)$\n",
    "\n",
    "| Term | d1 | d2 | d3 |\n",
    "|------|----------------|----------------|----------------|\n",
    "| the  | 0              | 0              | 0              |\n",
    "| cat  | 0.333×0.176=0.059 | 0              | 0.333×0.176=0.059 |\n",
    "| sat  | 0.333×0.176=0.059 | 0.333×0.176=0.059 | 0              |\n",
    "| dog  | 0              | 0.333×0.477=0.159 | 0              |\n",
    "| ate  | 0              | 0              | 0.333×0.477=0.159 |\n",
    "\n",
    "---\n",
    "\n",
    "### Final TF-IDF Table\n",
    "| Term | d1    | d2    | d3    |\n",
    "|------|-------|-------|-------|\n",
    "| the  | 0     | 0     | 0     |\n",
    "| cat  | 0.059 | 0     | 0.059 |\n",
    "| sat  | 0.059 | 0.059 | 0     |\n",
    "| dog  | 0     | 0.159 | 0     |\n",
    "| ate  | 0     | 0     | 0.159 |\n",
    "\n",
    "**Interpretation:**\n",
    "- Words like \"the\" (in every doc) get 0 weight in the manual calculation, but may get a small value in scikit-learn.\n",
    "- Words unique to a document (\"dog\", \"ate\") get the highest weight in that document.\n",
    "- Words in some but not all docs (\"cat\", \"sat\") get medium weight.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "- TF-IDF highlights words that are important to a document but not common across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1fb4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "      ate    cat    dog    sat  the\n",
      "d1  0.000  1.405  0.000  1.405  1.0\n",
      "d2  0.000  0.000  2.099  1.405  1.0\n",
      "d3  2.099  1.405  0.000  0.000  1.0\n"
     ]
    }
   ],
   "source": [
    "# Example: TF-IDF Calculation in Python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Example documents\n",
    "docs = [\n",
    "    \"the cat sat\",  # d1\n",
    "    \"the dog sat\",  # d2\n",
    "    \"the cat ate\"   # d3\n",
    "]\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None, use_idf=True, smooth_idf=False)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Get feature names and TF-IDF matrix\n",
    "features = vectorizer.get_feature_names_out()\n",
    "tfidf_matrix = X.toarray()\n",
    "\n",
    "# Show as a DataFrame for clarity\n",
    "df = pd.DataFrame(tfidf_matrix, columns=features, index=[\"d1\", \"d2\", \"d3\"])\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962beed0",
   "metadata": {},
   "source": [
    "# Count Vectorization (Bag-of-Words Model)\n",
    "\n",
    "**Definition:**\n",
    "Count Vectorization, also known as the Bag-of-Words (BoW) model, is a simple and widely used technique to convert text documents into numerical feature vectors. It represents each document by counting the number of times each word appears, ignoring grammar and word order but keeping multiplicity.\n",
    "\n",
    "- **Key idea:**\n",
    "  - Each unique word in the corpus becomes a feature (column).\n",
    "  - Each document is represented as a vector of word counts.\n",
    "  - The result is a sparse matrix where each row is a document and each column is a word from the vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Formula\n",
    "- For a document $d$ and term $t$:\n",
    "  - $\\text{Count}(t, d) = \\text{Number of times term } t \\text{ appears in document } d$\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Example Documents\n",
    "Suppose we have the same 3 documents:\n",
    "- d1: \"the cat sat\"\n",
    "- d2: \"the dog sat\"\n",
    "- d3: \"the cat ate\"\n",
    "\n",
    "### Step 1: Build the Vocabulary\n",
    "All unique terms: **the, cat, sat, dog, ate**\n",
    "\n",
    "### Step 2: Count Vector Table\n",
    "| Term | d1 | d2 | d3 |\n",
    "|------|----|----|----|\n",
    "| the  |  1 |  1 |  1 |\n",
    "| cat  |  1 |  0 |  1 |\n",
    "| sat  |  1 |  1 |  0 |\n",
    "| dog  |  0 |  1 |  0 |\n",
    "| ate  |  0 |  0 |  1 |\n",
    "\n",
    "- Each cell shows the number of times the word appears in the document.\n",
    "- Each document is now a vector: e.g., d1 = [1, 1, 1, 0, 0] (for [the, cat, sat, dog, ate])\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "- Count Vectorization is simple and fast, but does not consider word importance or frequency across documents (unlike TF-IDF).\n",
    "- It is often used as a baseline for text feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3edbd406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vector Matrix:\n",
      "    ate  cat  dog  sat  the\n",
      "d1    0    1    0    1    1\n",
      "d2    0    0    1    1    1\n",
      "d3    1    1    0    0    1\n"
     ]
    }
   ],
   "source": [
    "# Example: Count Vectorization in Python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Example documents\n",
    "docs = [\n",
    "    \"the cat sat\",  # d1\n",
    "    \"the dog sat\",  # d2\n",
    "    \"the cat ate\"   # d3\n",
    "]\n",
    "\n",
    "# Create the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Get feature names and count matrix\n",
    "features = vectorizer.get_feature_names_out()\n",
    "count_matrix = X.toarray()\n",
    "\n",
    "# Show as a DataFrame for clarity\n",
    "df = pd.DataFrame(count_matrix, columns=features, index=[\"d1\", \"d2\", \"d3\"])\n",
    "print(\"Count Vector Matrix:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca500e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
