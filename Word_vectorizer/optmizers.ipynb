{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427952ef",
   "metadata": {},
   "source": [
    "# What are Optimizers?\n",
    "\n",
    "Optimizers are algorithms used in machine learning and deep learning to adjust the parameters (weights and biases) of a model during training. Their main goal is to minimize the loss function, which measures how far the model's predictions are from the actual values. By updating the parameters in the right direction, optimizers help the model learn patterns from data and improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc8154",
   "metadata": {},
   "source": [
    "## Detailed Explanation of Optimizers\n",
    "\n",
    "### 1). First Order Optimizers\n",
    "First order optimizers are optimization algorithms that use only the first derivative (gradient) of the loss function with respect to the model parameters to update those parameters. These optimizers are called \"first order\" because they rely solely on gradient information, not on higher-order derivatives (like the Hessian matrix).\n",
    "\n",
    "- **Loss Function:** A loss function (also called cost function or objective function) measures how well the model's predictions match the actual data. The goal of training is to minimize this value. Common types include:\n",
    "    - **Mean Squared Error (MSE):** Used for regression tasks, measures the average squared difference between predicted and actual values.\n",
    "    - **Cross-Entropy Loss:** Used for classification tasks, measures the difference between the predicted probability distribution and the actual distribution.\n",
    "    - **Hinge Loss:** Used for support vector machines, penalizes predictions that are on the wrong side of the margin.\n",
    "\n",
    "- **Gradient:** The gradient is a vector of partial derivatives of the loss function with respect to each model parameter. It points in the direction of the steepest increase of the loss. By moving in the opposite direction of the gradient, we can minimize the loss. Types of gradients:\n",
    "    - **Batch Gradient:** Computed using the entire dataset.\n",
    "    - **Stochastic Gradient:** Computed using a single data point.\n",
    "    - **Mini-batch Gradient:** Computed using a small subset of the data.\n",
    "\n",
    "- **Parameter Update:** The optimizer updates the model parameters by subtracting a fraction (learning rate) of the gradient from the current parameters.\n",
    "\n",
    "**Examples of First Order Optimizers:**\n",
    "- **Gradient Descent:** Updates parameters using the gradient of the loss over the whole dataset.\n",
    "- **Stochastic Gradient Descent (SGD):** Updates parameters using the gradient from a single data point at a time.\n",
    "- **Mini-batch Gradient Descent:** Uses a small batch of data for each update, balancing speed and accuracy.\n",
    "- **Momentum:** Adds a fraction of the previous update to the current update, helping to accelerate convergence and avoid local minima.\n",
    "\n",
    "### 2).  Adaptive Optimizers\n",
    "Adaptive optimizers are advanced optimization algorithms that adjust the learning rate for each parameter individually based on the history of gradients. This allows the optimizer to adapt to the geometry of the loss surface, often leading to faster and more stable convergence.\n",
    "\n",
    "- **Learning Rate:** The learning rate is a hyperparameter that determines the size of the steps taken during optimization. Adaptive optimizers automatically adjust this for each parameter.\n",
    "- **Gradient Accumulation:** Adaptive optimizers keep track of past gradients (and sometimes squared gradients) to inform future updates.\n",
    "\n",
    "**Examples of Adaptive Optimizers:**\n",
    "- **AdaGrad:** Adapts the learning rate for each parameter based on the sum of the squares of all previous gradients. Works well for sparse data.\n",
    "- **RMSProp:** Modifies AdaGrad by using a moving average of squared gradients, preventing the learning rate from shrinking too much.\n",
    "- **Adam (Adaptive Moment Estimation):** Combines the ideas of Momentum and RMSProp, using moving averages of both gradients and squared gradients.\n",
    "- **AdaDelta:** An extension of AdaGrad that seeks to reduce its aggressive, monotonically decreasing learning rate.\n",
    "\n",
    "**Key Terms Explained:**\n",
    "- **Hyperparameter:** A parameter whose value is set before the learning process begins (e.g., learning rate, batch size).\n",
    "- **Convergence:** The process of approaching a minimum value of the loss function during training.\n",
    "- **Local Minimum:** A point where the loss is lower than at neighboring points, but not necessarily the lowest possible (global minimum).\n",
    "\n",
    "By understanding these optimizers and their components, you can choose the right optimization strategy for your machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c215e77",
   "metadata": {},
   "source": [
    "# Understanding Optimizers in Machine Learning\n",
    "\n",
    "## What is an Optimizer?\n",
    "An optimizer is an algorithm or method used in machine learning and deep learning to adjust the parameters (weights and biases) of a model in order to minimize the loss function. The optimizer updates the model parameters based on the gradients computed during backpropagation, helping the model learn from data and improve its predictions.\n",
    "\n",
    "## Types of Optimizers\n",
    "\n",
    "### 1. First Order Optimizers\n",
    "First order optimizers use only the first derivative (gradient) of the loss function to update the parameters. They are generally simple and efficient.\n",
    "**Examples:**\n",
    "- Batch Gradient Descent\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- Mini-batch Gradient Descent\n",
    "\n",
    "\n",
    "### 2. Adaptive Optimizers\n",
    "Adaptive optimizers adjust the learning rate for each parameter individually based on past gradients. They often lead to faster convergence and better performance in practice.\n",
    "**Examples:**\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam\n",
    "- AdaDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143de44a",
   "metadata": {},
   "source": [
    "# 1). First Order Optimizers\n",
    "## Batch Gradient Descent\n",
    "\n",
    "Batch Gradient Descent is an optimization algorithm used to train machine learning models. In this method, the entire training dataset is used to compute the gradient of the loss function with respect to the model parameters. The parameters are then updated based on this gradient.\n",
    "\n",
    "### How it Works:\n",
    "1. Calculate the predictions for all training examples using the current model parameters.\n",
    "2. Compute the loss (error) for all predictions compared to the actual values.\n",
    "3. Calculate the gradient (partial derivatives) of the loss function with respect to each parameter, using the entire dataset.\n",
    "4. Update the parameters by moving them in the direction that reduces the loss (opposite to the gradient).\n",
    "\n",
    "#### Characteristics:\n",
    "- Uses the whole dataset for each update, which can be slow for large datasets.\n",
    "- Provides a stable and accurate estimate of the gradient.\n",
    "- Can get stuck in local minima or saddle points.\n",
    "\n",
    "#### Formula:\n",
    "For parameter $\\theta$ and learning rate $\\alpha$:\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\nabla J(\\theta)$$\n",
    "where $\\nabla J(\\theta)$ is the gradient of the loss function $J$ with respect to $\\theta$, computed over the entire dataset.\n",
    "\n",
    "#### Pros:\n",
    "- Stable convergence\n",
    "- Accurate gradient estimation\n",
    "\n",
    "#### Cons:\n",
    "- Slow for large datasets\n",
    "- High memory usage\n",
    "\n",
    "Batch Gradient Descent is best suited for smaller datasets where computation time and memory are not major concerns.\n",
    "\n",
    "### Example: Batch Gradient Descent with Epochs\n",
    "\n",
    "#### What is an Epoch?\n",
    "An **epoch** is one complete pass through the entire training dataset during the training process. In other words, when every sample in the dataset has been used once to update the model parameters, one epoch is completed. Training a model usually involves multiple epochs to allow the model to learn better from the data.\n",
    "\n",
    "###3 Example Scenario\n",
    "Suppose you have a dataset with 1000 samples. If you use batch gradient descent, the model will:\n",
    "- Use all 1000 samples to compute the gradient and update the parameters once. This is one epoch.\n",
    "- Repeat this process for several epochs (e.g., 10 epochs), so the model sees the entire dataset 10 times, updating the parameters after each pass.\n",
    "\n",
    "**Training Process:**\n",
    "1. **Epoch 1:**\n",
    "    - Compute predictions for all 1000 samples.\n",
    "    - Calculate the loss and gradients using all samples.\n",
    "    - Update the model parameters once.\n",
    "2. **Epoch 2:**\n",
    "    - Repeat the process with the updated parameters.\n",
    "3. **Continue for the desired number of epochs.**\n",
    "\n",
    "#### Why Use Multiple Epochs?\n",
    "- The model may not learn enough from just one pass through the data.\n",
    "- Multiple epochs allow the model to gradually minimize the loss and improve accuracy.\n",
    "\n",
    "#### Visualization:\n",
    "\n",
    "| Epoch | Loss (Example) |\n",
    "|-------|---------------|\n",
    "|   1   |     0.95      |\n",
    "|   2   |     0.80      |\n",
    "|   3   |     0.65      |\n",
    "|  ...  |     ...       |\n",
    "|  10   |     0.20      |\n",
    "\n",
    "As the number of epochs increases, the loss typically decreases, showing that the model is learning from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_epochs):\n",
    "    params_grad=evaluate_gradient(loss_function,data,params)\n",
    "    params=params-learning_rate * params_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af871b4f",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is an optimization algorithm used to train machine learning models. Unlike batch gradient descent, which uses the entire dataset to compute the gradient, SGD updates the model parameters using only a single randomly selected data point at each step.\n",
    "\n",
    "#### How it Works:\n",
    "1. Shuffle the training dataset.\n",
    "2. For each training example:\n",
    "    - Compute the prediction using the current model parameters.\n",
    "    - Calculate the loss for this single example.\n",
    "    - Compute the gradient of the loss with respect to the model parameters (using only this example).\n",
    "    - Update the parameters immediately based on this gradient.\n",
    "3. Repeat the process for multiple epochs (passes through the dataset).\n",
    "\n",
    "#### Characteristics:\n",
    "- Updates parameters more frequently (after each example), which can lead to faster initial learning.\n",
    "- Introduces more noise in the updates, which can help escape local minima but may cause the loss to fluctuate.\n",
    "- Well-suited for large datasets and online learning.\n",
    "\n",
    "#### Formula:\n",
    "For parameter $\\theta$ and learning rate $\\alpha$, using a single example $(x_i, y_i)$:\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\nabla J(\\theta; x_i, y_i)$$\n",
    "where $\\nabla J(\\theta; x_i, y_i)$ is the gradient of the loss function $J$ with respect to $\\theta$, computed for the $i$-th example.\n",
    "\n",
    "#### Example Scenario\n",
    "Suppose you have a dataset with 1000 samples. In SGD:\n",
    "- The model picks one sample at a time, computes the gradient, and updates the parameters immediately.\n",
    "- After all 1000 samples have been used once, one epoch is completed.\n",
    "- This process is repeated for several epochs (e.g., 10 epochs).\n",
    "\n",
    "**Training Process:**\n",
    "1. **Epoch 1:**\n",
    "    - For each sample (from 1 to 1000):\n",
    "        - Compute prediction, loss, gradient, and update parameters.\n",
    "2. **Epoch 2:**\n",
    "    - Repeat the process with the updated parameters.\n",
    "3. **Continue for the desired number of epochs.**\n",
    "\n",
    "#### Visualization:\n",
    "\n",
    "| Epoch | Loss (Example) |\n",
    "|-------|---------------|\n",
    "|   1   |     1.10      |\n",
    "|   2   |     0.85      |\n",
    "|   3   |     0.70      |\n",
    "|  ...  |     ...       |\n",
    "|  10   |     0.25      |\n",
    "\n",
    "The loss may fluctuate more from step to step, but generally decreases over epochs as the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e1eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_epochs):\n",
    "    np.random.shuffle(data)\n",
    "    for example in data:\n",
    "        params_grad = evaluate_gradient(loss_function, example, params)\n",
    "        params = params - learning_rate * params_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce6bb3",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent\n",
    "\n",
    "Mini-Batch Gradient Descent is an optimization algorithm that combines the advantages of both batch and stochastic gradient descent. Instead of using the entire dataset (batch) or a single data point (stochastic) to compute the gradient, it uses a small random subset of the data called a \"mini-batch.\"\n",
    "\n",
    "#### How it Works:\n",
    "1. Shuffle the training dataset.\n",
    "2. Divide the dataset into small groups called mini-batches (e.g., 32, 64, or 128 samples per mini-batch).\n",
    "3. For each mini-batch:\n",
    "    - Compute predictions for all samples in the mini-batch.\n",
    "    - Calculate the loss and gradients using only the mini-batch.\n",
    "    - Update the model parameters based on the mini-batch gradient.\n",
    "4. Repeat the process for all mini-batches in the dataset (one epoch).\n",
    "5. Repeat for multiple epochs.\n",
    "\n",
    "\n",
    "#### Characteristics:\n",
    "- Balances the efficiency and stability of batch and stochastic methods.\n",
    "- Faster convergence than batch gradient descent and less noisy than stochastic gradient descent.\n",
    "- Well-suited for large datasets and can take advantage of parallel hardware (like GPUs).\n",
    "\n",
    "#### Formula:\n",
    "For parameter $\\theta$ and learning rate $\\alpha$, using a mini-batch $B$ of $m$ samples:\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\frac{1}{m} \\sum_{i \\in B} \\nabla J(\\theta; x_i, y_i)$$\n",
    "where $\\nabla J(\\theta; x_i, y_i)$ is the gradient of the loss function for the $i$-th sample in the mini-batch.\n",
    "\n",
    "#### Example Scenario\n",
    "Suppose you have a dataset with 1000 samples and choose a mini-batch size of 100:\n",
    "- The dataset is divided into 10 mini-batches (each with 100 samples).\n",
    "- For each mini-batch, compute the gradient and update the parameters.\n",
    "- After all 10 mini-batches are processed, one epoch is completed.\n",
    "- Repeat for several epochs (e.g., 10 epochs).\n",
    "\n",
    "**Training Process:**\n",
    "1. **Epoch 1:**\n",
    "    - For each mini-batch (1 to 10):\n",
    "        - Compute predictions, loss, gradient, and update parameters.\n",
    "2. **Epoch 2:**\n",
    "    - Repeat the process with updated parameters.\n",
    "3. **Continue for the desired number of epochs.**\n",
    "\n",
    "#### Visualization:\n",
    "\n",
    "| Epoch | Loss (Example) |\n",
    "|-------|---------------|\n",
    "|   1   |     0.90      |\n",
    "|   2   |     0.75      |\n",
    "|   3   |     0.60      |\n",
    "|  ...  |     ...       |\n",
    "|  10   |     0.18      |\n",
    "\n",
    "Mini-batch gradient descent is the most commonly used method in deep learning due to its efficiency and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd49ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_epochs):\n",
    "    np.random.shuffle(data)\n",
    "    for batch in get_batches(data, batch_size=50): # here batch_size means 50 samples\n",
    "        params_grad = evaluate_gradient(loss_function, batch, params)\n",
    "        params = params - learning_rate * params_grad   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
