{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2807caac",
   "metadata": {},
   "source": [
    "# Activation Functions in Neural Networks\n",
    "\n",
    "Activation functions are mathematical formulas that determine the output of a neuron in a neural network. They introduce non-linearity, enabling networks to learn complex patterns. Here’s a structured summary of the most important activation functions, their formulas, detailed descriptions, advantages, and limitations.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Binary Step\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x) = \\begin{cases}\n",
    "    0, & x < 0 \\\\\n",
    "    1, & x \\geq 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- **Description:** The binary step function outputs either 0 or 1 depending on whether the input is below or above a threshold (usually 0). It is used for simple decision-making tasks, such as binary classification in perceptrons.\n",
    "- **Why Use:** Simple, useful for binary classification (yes/no).\n",
    "- **Limitations:** Not differentiable, can’t train deep networks, too rigid for complex tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Linear\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x) = x\n",
    "  $$\n",
    "- **Description:** The linear activation function outputs the input directly. It is used in regression tasks where the output can be any real value. However, stacking linear layers results in a linear function, so it cannot model complex patterns.\n",
    "- **Why Use:** Easy, keeps values unchanged. Used in regression output layers.\n",
    "- **Limitations:** No non-linearity, can’t model complex patterns, not suitable for deep networks.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Sigmoid\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "- **Description:** The sigmoid function squashes input values into the range (0, 1). It is often used in the output layer for binary classification, as it can represent probabilities. However, it suffers from vanishing gradients for large positive or negative inputs.\n",
    "- **Why Use:** Outputs between (0,1). Good for probabilities. Smooth and differentiable.\n",
    "- **Limitations:** Vanishing gradient for large |x|, slow to train, not zero-centered, can cause slow convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Tanh (Hyperbolic Tangent)\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "  $$\n",
    "- **Description:** The tanh function squashes input values into the range (–1, 1). It is zero-centered, making optimization easier than sigmoid. Commonly used in hidden layers of neural networks.\n",
    "- **Why Use:** Outputs between (–1,1). Zero-centered. Better for hidden layers than sigmoid.\n",
    "- **Limitations:** Still suffers vanishing gradient for large |x|, can slow down learning.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ReLU (Rectified Linear Unit)\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x) = \\max(0, x)\n",
    "  $$\n",
    "- **Description:** ReLU outputs the input if it is positive, otherwise 0. It is the most widely used activation in deep learning because it is simple and helps mitigate the vanishing gradient problem.\n",
    "- **Why Use:** Simple, fast, avoids vanishing gradient (mostly). Most widely used in hidden layers.\n",
    "- **Limitations:** “Dying ReLU” problem (neurons stuck at 0 for negative inputs), not zero-centered.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Leaky ReLU\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x) = \\begin{cases}\n",
    "    x, & x \\geq 0 \\\\\n",
    "    \\alpha x, & x < 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "  (α ≈ 0.01)\n",
    "- **Description:** Leaky ReLU allows a small, non-zero gradient when the input is negative, fixing the dying ReLU problem. The slope α is a small constant.\n",
    "- **Why Use:** Fixes dying ReLU by allowing small negative slope.\n",
    "- **Limitations:** α is fixed, may not be optimal for all tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Parametric ReLU (PReLU)\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x) = \\begin{cases}\n",
    "    x, & x \\geq 0 \\\\\n",
    "    a x, & x < 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "  (a is learned during training)\n",
    "- **Description:** PReLU generalizes Leaky ReLU by making the negative slope a learnable parameter, allowing the network to adapt the slope during training.\n",
    "- **Why Use:** Learns the negative slope automatically, more flexible.\n",
    "- **Limitations:** More parameters, risk of overfitting if data is limited.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Exponential Linear Unit (ELU)\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x) = \\begin{cases}\n",
    "    x, & x > 0 \\\\\n",
    "    \\alpha (e^x - 1), & x \\leq 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- **Description:** ELU uses an exponential function for negative inputs, which helps keep mean activations close to zero and improves learning speed.\n",
    "- **Why Use:** Negative outputs help keep mean near 0. Smooth gradients, helps convergence.\n",
    "- **Limitations:** More expensive to compute (uses exponentials), α must be set.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Softmax\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "  $$\n",
    "- **Description:** Softmax converts a vector of raw scores (logits) into probabilities that sum to 1. Used in the output layer for multi-class classification.\n",
    "- **Why Use:** Turns outputs into probabilities (sums to 1). Great for multi-class classification.\n",
    "- **Limitations:** Sensitive to large inputs (can overflow), not used in hidden layers.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Softplus\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x) = \\ln(1 + e^x)\n",
    "  $$\n",
    "- **Description:** Softplus is a smooth, differentiable approximation to ReLU. It never outputs exactly zero, so it avoids the dying ReLU problem.\n",
    "- **Why Use:** Smooth, differentiable version of ReLU. No dying ReLU problem.\n",
    "- **Limitations:** Slower to compute, doesn’t fully solve vanishing gradient.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Swish\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x) = x \\cdot \\sigma(x) = x \\cdot \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "- **Description:** Swish is a smooth, non-monotonic function that often outperforms ReLU in deep networks. It allows small negative values and is differentiable everywhere.\n",
    "- **Why Use:** Smooth, avoids dying ReLU. Better performance in some deep nets.\n",
    "- **Limitations:** More expensive than ReLU, not always better for all tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. GELU (Gaussian Error Linear Unit)\n",
    "- **Formula:**\n",
    "  $$\n",
    "  f(x) = x \\cdot \\Phi(x)\n",
    "  $$\n",
    "  (Φ(x) = CDF of standard normal distribution)\n",
    "- **Description:** GELU weights inputs by their value and the probability that a standard normal variable is less than that value. Used in modern NLP models (e.g., BERT, GPT).\n",
    "- **Why Use:** Used in Transformers (BERT, GPT). Smooth, probabilistic gating, helps with deep architectures.\n",
    "- **Limitations:** Computationally heavy, more complex to implement.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Activation Function | Formula | Why Use (Advantages) | Limitations |\n",
    "|--------------------|---------|---------------------|-------------|\n",
    "| Binary Step | $f(x)=\\begin{cases}0, x<0 \\\\ 1, x\\geq0\\end{cases}$ | Simple, binary classification | Not differentiable, too rigid |\n",
    "| Linear | $f(x)=x$ | Easy, regression output | No non-linearity |\n",
    "| Sigmoid | $f(x)=\\frac{1}{1+e^{-x}}$ | Probabilities, smooth | Vanishing gradient, not zero-centered |\n",
    "| Tanh | $f(x)=\\tanh(x)$ | Zero-centered, [-1,1] | Vanishing gradient |\n",
    "| ReLU | $f(x)=\\max(0,x)$ | Fast, avoids vanishing gradient | Dying ReLU |\n",
    "| Leaky ReLU | $f(x)=\\max(\\alpha x, x)$ | Fixes dying ReLU | α fixed, not optimal |\n",
    "| PReLU | $f(x)=\\max(ax, x)$ | Learns slope | More params, overfitting |\n",
    "| ELU | $f(x)=\\begin{cases}x, x>0 \\\\ \\alpha(e^x-1), x\\leq0\\end{cases}$ | Smooth, mean near 0 | Expensive |\n",
    "| Softmax | $f(x_i)=\\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | Probabilities, multi-class | Overflow risk |\n",
    "| Softplus | $f(x)=\\ln(1+e^x)$ | Smooth ReLU | Slower |\n",
    "| Swish | $f(x)=x\\cdot\\sigma(x)$ | Smooth, flexible | Expensive |\n",
    "| GELU | $f(x)=x\\cdot\\Phi(x)$ | Used in NLP, smooth | Heavy |\n",
    "\n",
    "---\n",
    "\n",
    "> **Tip:** Use ReLU or its variants for hidden layers, Softmax for multi-class output, Sigmoid for binary output, and Linear for regression output.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
